


<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>

  <meta name="viewport" content="width=500">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/favicon.ico">
  <title>Manasi Sharma, Stanford University</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link rel="shortcut icon" type="image/jpg" href="favicon2.jpg">
</head>

<body>
  <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <!--<name>Manasi Sharma</name>-->
                <head>
                  <h1>Manasi Sharma</h1>
                </head>
              </p>
              <!-- About -->
              <p>
                <b>Hi there!</b> I am a second year MS student in Computer Science at Stanford University on the Artificial Intelligence track. I am currently member of the <a href="https://svl.stanford.edu/">Stanford Vision Lab</a> under Prof. Fei-Fei Li and Prof. Jiajun Wu, where I work on the <a href="https://sites.google.com/view/behavior-1k">BEHAVIOR</a> project in robotic simulation benchmarking and the <a href="https://github.com/corgiTrax/AIDA">AIDA</a> (Attention-driven data augmentation) project which aims to improve image classification performance using a saliency-map based augmentation. I consider myself a generalist in AI and am interested in the broad domains of Deep Learning, Computer Vision, Explainability, Reinforcement Learning & Decision Making, Graph Neural Networks.
                <!-- I am also very interested in the applications of perception-based systems and AI to a variety of scenarios, from Astronomy to Autonomous Systems. -->
            </p>
            <p>
                I graduated in 2021 from Columbia University with a Bachelor's degree in Computer Science and Physics. As an undergrad, I was supervised by <a href="https://www.cs.columbia.edu/~djhsu/"> Prof. Daniel Hsu</a> and <a href="https://datascience.columbia.edu/people/zoltan-haiman/"> Prof. Zoltan Haiman</a> on interpreting astrophysical deep learning models for weak lensing using an array of saliency map methods. I started off my undergraduate program with a broad interest in computational physics and first pursued the direction of AI at Caltech in 2019, under <a href="https://sites.astro.caltech.edu/~mansi/"> Prof. Mansi Kasliwal </a> in which I developed a real/bogus image astrophysical source classifier for the IR-Gattini telescope that performed so well that it was permanently included in the telescope's data processing pipeline. 
            </p>
            <p>
              As I entered graduate school, unfulfilled by the relatively limited impact astrophysics can have on current-life and real-world affairs, my interest in AI broadened to 
              include more real-world applications that had a greater focus on human and human-computer interaction, such as robotics and AV. I am now looking for full-time roles (starting 2023) and am very excited to learn about opportunities for the application of deep-learning and/or perception-based systems to a variety of scenarios!
            </p>
           
              <!-- Contact details -->
              <p align=center>
                <a href="mailto:manasis@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="Resume_Manasi_Sharma.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/manasi-sharma">Github</a> &nbsp/&nbsp
                <a href="https://bit.ly/3PsYx2c">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://arxiv.org/a/jena_r_1.html">ArXiv</a>&nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/manasi1/">LinkedIn </a> &nbsp/&nbsp
                <a href="https://twitter.com/ManasiSharma_">Twitter </a>
              </p>

            </td>
            <td width="83%">
            <img width="100%" style="border-radius: 100%;" src="other_headshot4.jpg">

              </td>
          </tr>
        </table>

        <!-- Professional Experience -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                    <!-- <heading>Updates</heading> -->
                    <head>
                      <h1>Professional / Research Experience</h1>
                    </head>
                    <p>
                        <ul id="news">
                          <td width="25%">
                            <img width="90%" src="ijhff.jpg" alt="obj">
                           </td>
                             <br>
                            <head>
                              <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                              <h5>[June 2022 -- Sep 2022]</h5>
                            </head>

                            <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using the 'SimpleView' supervised learning algorithm, which projects a 3D point-cloud onto the 6 2D frames and passes it through a CNN (ResNet backbone). </li>
                            <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                            <li> Experimented with a number of architechtures including Graph-based methods like Grid-GCN and point-wise MLP methods like PointNet++, finally settling on the lightweight convolution based method SimpleView. </li>
                            <li> LiDAR point-cloud classification is exceedingly difficult and I encountered numerous challenges-- I was finally able to improve model performance and speed by sparisfying the network, using saliency-based weight-freezing and training on hard-negative samples. </li>
                            <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                            <li> Video for car classification linked <a href="demot94.mp4">here</a>. </li>
                            
                            <head>
                              <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                              <h5>[Oct 2021 -- present]</h5>
                            </head>

                            <b> BEHAVIOR Project:</b>
                            <li> Led the development of the Knowledgebase for iGibson and BEHAVIOR-1K, an ImageNet-scale robotic simulation benchmark with a specifc-focus on human-relevant design. Paper accepted to the Conference of Robotics and Learning '22. </li>
                            <li> Mobilized ~20 crowd-workers to categorize ~5000 "how-to" articles and used zero-shot Natural Language Processing techniques with GPT-3 to annotate the Virtual Reality videos and generate >97% quality activity definitions in a predicate logic-based language. </li>
                            <b> ADDA (Attention-driven data augmentation):</b>
                            <li> Oversaw 5 person team project on 'Modulated Attention Dropout' technique to allow for better generalization of RL policies through task-importance aware dataset augmentation. Results showed a 2% increase on baseline Behavioral Cloning results. </li>
                            
                            <head>
                              <h3>Columbia University, Data Science Institute</a> </h3>
                              <h5>[Sep 2019 -- June 2021]</h5>
                            </head>

                            <li> Under the guidance of Prof.'s Daniel Hsu and Zoltan Haiman, worked on targeting the "explainability" & trustworthiness of neural networks in the more traditional field of Astronomy. </li>
                            <li> Discovered that 89% of the output of a popular neural network that uses gravitational lensing maps to predict cosmological parameters (omega_m and sigma_8) was counterintuitively attributable to negative image regions (voids, black holes, etc.) as opposed to the bright image regions (stars, galaxies, etc.). Published results in APS Physical Review '20. </li>
                            <li> Also ran sanity checks on a number of popular saliency methods and found that gradient-based methods (eg. Grad-CAM / Input x Gradients, etc.) were most robust to model parameters. </li>

                            <head>
                              <h3>California Institute of Technology, Division of Physics, Mathematics and Astronomy</a> </h3>
                              <h5>[Jun 2019 -- Aug 2019]</h5>
                            </head>

                            <li> Under the guidance of Prof. Mansi Kasliwal at Caltech, pioneered the development of a flagship CNN-based real/bogus image classification system for Caltech's Gattini-IR Telescope using TensorFlow (link), which achieved ~97.5% accuracy on thousands of cosmic transient sources, and published results in PASP '19. </li>
                            <li> The model worked so well that it was deployed the model in the Telescope's data processing pipeline (still active), replacing the manual classification process. </li>
                            <li> I also used the results of the model to identify high-confidence transient sources that I performed optical followup on by operating the 200-inch telescope at the Palomar Observatory in Southern California. </li>
                          
                            <head>
                              <h3>Columbia University, Department of Physics</a> </h3>
                              <h5>[Jun 2018 -- May 2019]</h5>
                            </head>

                            <li> Worked with Prof. Charles Hailey in the NuSTAR Group as a Laidlaw Research Intern. </li>
                            <li> Modeled/analyzed data from NASA's NuSTAR telescope for 'AM Her' & 'HU Aqr' sources to determine key parameters such as temperature and periodicity. </li>

                          </ul>
                      </p>
                  </td>
              </tr>
          </table>

        <!-- Publications -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <!-- <heading>Publications</heading> -->
              <head>
                <h1>Publications</h1>
              </head>
            </td>
          </tr>
        </table>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%">
             <img width="90%" src="ijhff.jpg" alt="obj">
            </td>
              <br>
             <td valign="middle" width="75%">
               <papertitle>BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation</papertitle>

            <br>
                             <strong>Manasi Sharma</strong>,
              <a href="https://sites.google.com/site/appcfdlab">Abhilash J. Chandy</a>
              <br>
              <em>Conference on Robot Learning (CoRL), 2022</em>
              <br>
                      <a href="/~rahul29/Publications/LES_forced_stratified_turbulence.pdf">Paper link</a>
        <!-- <a href="data/ipmi2019_slides.pdf">slides</a> -->
              <p></p>
              <p>We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics, motivated by the results of an extensive survey on `what do you want robots to do for you?'. It includes the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 3,000 objects annotated with physical and semantic properties. It also includes OmniGibson, a novel simulator that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids.</p>
            </td>
          </tr>

        </table>

        <!-- Teaching Experience -->        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                  <!-- <heading>Updates</heading> -->
                  <head>
                    <h1>Teaching Experience</h1>
                  </head>
                  <p>
                      <ul id="news">
                          <head>
                            <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                            <h5>[June 2022 -- Sep 2022]</h5>
                          </head>

                          <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using the 'SimpleView' supervised learning algorithm, which projects a 3D point-cloud onto the 6 2D frames and passes it through a CNN (ResNet backbone). </li>
                          <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                          <li> Experimented with a number of architechtures including Graph-based methods like Grid-GCN and point-wise MLP methods like PointNet++, finally settling on the lightweight convolution based method SimpleView. </li>
                          <li> LiDAR point-cloud classification is exceedingly difficult and I encountered numerous challenges-- I was finally able to improve model performance and speed by sparisfying the network, using saliency-based weight-freezing and training on hard-negative samples. </li>
                          <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                          <li> Video for car classification linked <a href="demot94.mp4">here</a>. </li>
                          
                          <head>
                            <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                            <h5>[Oct 2021 -- present]</h5>
                          </head>
                        </ul>
                    </p>
                </td>
            </tr>
        </table>

        <!-- Blog -->   
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                  <!-- <heading>Updates</heading> -->
                  <head>
                    <h1>Blog</h1>
                  </head>
                  <p>
                      <ul id="news">
                          <head>
                            <h3>Blog </a> </h3>
                            <h5>[June 2022 -- Sep 2022]</h5>
                          </head>

                          <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using the 'SimpleView' supervised learning algorithm, which projects a 3D point-cloud onto the 6 2D frames and passes it through a CNN (ResNet backbone). </li>
                          <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                          <li> Experimented with a number of architechtures including Graph-based methods like Grid-GCN and point-wise MLP methods like PointNet++, finally settling on the lightweight convolution based method SimpleView. </li>
                          <li> LiDAR point-cloud classification is exceedingly difficult and I encountered numerous challenges-- I was finally able to improve model performance and speed by sparisfying the network, using saliency-based weight-freezing and training on hard-negative samples. </li>
                          <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                          <li> Video for car classification linked <a href="demot94.mp4">here</a>. </li>
                          
                          <head>
                            <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                            <h5>[Oct 2021 -- present]</h5>
                          </head>
                        </ul>
                    </p>
                </td>
            </tr>
        </table>


      <!-- Honors -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                <!-- <heading>Updates</heading> -->
                <head>
                  <h1>Teaching Experience</h1>
                </head>
                <p>
                    <ul id="news">
                        <head>
                          <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                          <h5>[June 2022 -- Sep 2022]</h5>
                        </head>

                        <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using the 'SimpleView' supervised learning algorithm, which projects a 3D point-cloud onto the 6 2D frames and passes it through a CNN (ResNet backbone). </li>
                        <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                        <li> Experimented with a number of architechtures including Graph-based methods like Grid-GCN and point-wise MLP methods like PointNet++, finally settling on the lightweight convolution based method SimpleView. </li>
                        <li> LiDAR point-cloud classification is exceedingly difficult and I encountered numerous challenges-- I was finally able to improve model performance and speed by sparisfying the network, using saliency-based weight-freezing and training on hard-negative samples. </li>
                        <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                        <li> Video for car classification linked <a href="demot94.mp4">here</a>. </li>
                        
                        <head>
                          <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                          <h5>[Oct 2021 -- present]</h5>
                        </head>
                      </ul>
                  </p>
              </td>
          </tr>
      </table>


            <!-- Honors -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                <!-- <heading>Updates</heading> -->
                <head>
                  <h1>Teaching Experience</h1>
                </head>
                <p>
                    <ul id="news">
                        <head>
                          <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                          <h5>[June 2022 -- Sep 2022]</h5>
                        </head>

                        <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using the 'SimpleView' supervised learning algorithm, which projects a 3D point-cloud onto the 6 2D frames and passes it through a CNN (ResNet backbone). </li>
                        <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                        <li> Experimented with a number of architechtures including Graph-based methods like Grid-GCN and point-wise MLP methods like PointNet++, finally settling on the lightweight convolution based method SimpleView. </li>
                        <li> LiDAR point-cloud classification is exceedingly difficult and I encountered numerous challenges-- I was finally able to improve model performance and speed by sparisfying the network, using saliency-based weight-freezing and training on hard-negative samples. </li>
                        <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                        <li> Video for car classification linked <a href="demot94.mp4">here</a>. </li>
                        
                        <head>
                          <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                          <h5>[Oct 2021 -- present]</h5>
                        </head>
                      </ul>
                  </p>
              </td>
          </tr>
      </table>

      <!-- Extra-curriculars -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                <!-- <heading>Updates</heading> -->
                <head>
                  <h1>Teaching Experience</h1>
                </head>
                <p>
                    <ul id="news">
                        <head>
                          <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                          <h5>[June 2022 -- Sep 2022]</h5>
                        </head>

                        <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using the 'SimpleView' supervised learning algorithm, which projects a 3D point-cloud onto the 6 2D frames and passes it through a CNN (ResNet backbone). </li>
                        <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                        <li> Experimented with a number of architechtures including Graph-based methods like Grid-GCN and point-wise MLP methods like PointNet++, finally settling on the lightweight convolution based method SimpleView. </li>
                        <li> LiDAR point-cloud classification is exceedingly difficult and I encountered numerous challenges-- I was finally able to improve model performance and speed by sparisfying the network, using saliency-based weight-freezing and training on hard-negative samples. </li>
                        <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                        <li> Video for car classification linked <a href="demot94.mp4">here</a>. </li>
                        
                        <head>
                          <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                          <h5>[Oct 2021 -- present]</h5>
                        </head>
                      </ul>
                  </p>
              </td>
          </tr>
      </table>

      <!-- Media -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                <!-- <heading>Updates</heading> -->
                <head>
                  <h1>Teaching Experience</h1>
                </head>
                <p>
                    <ul id="news">
                        <head>
                          <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                          <h5>[June 2022 -- Sep 2022]</h5>
                        </head>

                        <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using the 'SimpleView' supervised learning algorithm, which projects a 3D point-cloud onto the 6 2D frames and passes it through a CNN (ResNet backbone). </li>
                        <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                        <li> Experimented with a number of architechtures including Graph-based methods like Grid-GCN and point-wise MLP methods like PointNet++, finally settling on the lightweight convolution based method SimpleView. </li>
                        <li> LiDAR point-cloud classification is exceedingly difficult and I encountered numerous challenges-- I was finally able to improve model performance and speed by sparisfying the network, using saliency-based weight-freezing and training on hard-negative samples. </li>
                        <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                        <li> Video for car classification linked <a href="demot94.mp4">here</a>. </li>
                        
                        <head>
                          <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                          <h5>[Oct 2021 -- present]</h5>
                        </head>
                      </ul>
                  </p>
              </td>
          </tr>
      </table>


</body>

</html>
