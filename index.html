


<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>

  <meta name="viewport" content="width=500">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/favicon.ico">
  <title>Manasi Sharma, Stanford University</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link rel="shortcut icon" type="image/jpg" href="favicon2.jpg">
</head>

<body>
  <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <!--<name>Manasi Sharma</name>-->
                <head>
                  <h1>Manasi Sharma</h1>
                </head>
              </p>
              <!-- About -->
              <p>
                <b>Hi there!</b> I am a second year MS student in Computer Science at Stanford University on the Artificial Intelligence track. I am currently member of the <a href="https://svl.stanford.edu/">Stanford Vision Lab</a> under Prof. Fei-Fei Li and Prof. Jiajun Wu, where I work on the <a href="https://sites.google.com/view/behavior-1k">BEHAVIOR</a> project in robotic simulation benchmarking and the <a href="https://github.com/corgiTrax/AIDA">AIDA</a> (Attention-driven data augmentation) project which aims to improve image classification performance using a saliency-map based augmentation. I consider myself a generalist in AI and am interested in the broad domains of Deep Learning, Computer Vision, Explainability, Reinforcement Learning & Decision Making, Graph Neural Networks.
                <!-- I am also very interested in the applications of perception-based systems and AI to a variety of scenarios, from Astronomy to Autonomous Systems. -->
            </p>
            <p>
                I graduated in 2021 from Columbia University with a Bachelor's degree in Computer Science and Physics. As an undergrad, I was supervised by <a href="https://www.cs.columbia.edu/~djhsu/"> Prof. Daniel Hsu</a> and <a href="https://datascience.columbia.edu/people/zoltan-haiman/"> Prof. Zoltan Haiman</a> on interpreting astrophysical deep learning models for weak lensing using an array of saliency map methods. I started off my undergraduate program with a broad interest in computational physics and first pursued the direction of AI at Caltech in 2019, under <a href="https://sites.astro.caltech.edu/~mansi/"> Prof. Mansi Kasliwal </a> in which I developed a real/bogus image astrophysical source classifier for the IR-Gattini telescope that performed so well that it was permanently included in the telescope's data processing pipeline.
         </p>

              <!-- Contact details -->
              <p align=center>
                <a href="mailto:manasis@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="Resume_Manasi_Sharma.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/manasi-sharma">Github</a> &nbsp/&nbsp
                <a href="https://bit.ly/3PsYx2c">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://arxiv.org/a/jena_r_1.html">ArXiv</a>&nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/manasi1/">LinkedIn </a> &nbsp/&nbsp
                <a href="https://twitter.com/ManasiSharma_">Twitter </a>
              </p>

            </td>
            <td width="83%">
            <img width="100%" style="border-radius: 100%;" src="other_headshot4.jpg">

              </td>
          </tr>
        </table>

        <!-- news updates -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                    <!-- <heading>Updates</heading> -->
                    <head>
                      <h1>Professional / Research Experience</h1>
                    </head>
                    <p>
                        <ul id="news">
                            <head>
                              <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                              <h5>[June 2022 -- present]</h5>
                            </head>
                            <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using a supervised learning algorithm based on Simple View. </li>
                            <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                            <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                            <li> [Video will be linked here at the conclusion of the internship]. </li>
                            <li> Extension: platform for occlusion of any point cloud / freezing of weights based on saliency / object orientation detection. </li>
                            
                            <head>
                              <h3>Stanford University <a href="https://svl.stanford.edu/"> Stanford Vision Laboratory </a> </h3>
                              <h5>[Oct 2021 -- present]</h5>
                            </head>
                            <li>  Collecting, annotating (using GPT-3), and verifying Virtual Reality demos, to simulate 500+ household activities as part of the iGibson and the
                              BEHAVIOR challenge projects. The goal is to create a curated benchmark dataset, analogous to ImageNet, to support 
                              research in large-scale training of robotic agents. </li>
                            
                            <head>
                              <h3>Nissan-Renault-Mitsubishi: Alliance Innovation Laboratory <a href="https://www.ail-sv.com/"> (AIL-SV) </a> </h3>
                              <h5>[June 2022 -- present]</h5>
                            </head>
                            <li> Implemented LiDAR point-cloud classification of for Autonomous Systems using a supervised learning algorithm based on Simple View. </li>
                            <li> The model was able to classify cars, pedestrians, cyclists and vegetation with over 95% accuracy on real-world data and within the 10 Hz LiDAR processor timeframe. </li>
                            <li> <b> The model will be deployed in Nissan Autonomous Vehicles beginning October '22.</b> </li>
                            <li> [Video will be linked here at the conclusion of the internship]. </li>
                            <li> Extension: platform for occlusion of any point cloud / freezing of weights based on saliency / object orientation detection. </li>
                        </ul>
                    </p>
                </td>
            </tr>
        </table>

</body>

</html>
